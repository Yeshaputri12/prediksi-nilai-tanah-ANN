# -*- coding: utf-8 -*-
"""Prediksi-nilai-tanah-ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rOtqi32dguDBlShxdPTjQMVdMwbaN7h0
"""

# ======================= IMPORT LIBRARY =======================
import pandas as pd
import numpy as np
import random
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from tensorflow.keras.callbacks import EarlyStopping

# ======================= SETUP SEED =======================
seed = 42
np.random.seed(seed)
tf.random.set_seed(seed)
random.seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)

# ======================= LOAD DATA =======================
data = pd.read_csv('/content/drive/MyDrive/TUGAS AKHIR/DATA/Variabel_ANN.csv')

# ======================= ENCODING =======================
zonasi_mapping = {
    "lindung dan sempadan": 1,
    "pertanian": 2,
    "permukiman": 3,
    "industri": 4,
    "perdagangan dan jasa": 5
}
kls_jalan_mapping = {
    "jalan setapak": 1,
    "jalan lokal sekunder": 2,
    "jalan lokal primer": 3,
    "jalan kolektor sekunder": 4,
    "jalan kolektor primer": 5,
    "jalan arteri sekunder": 6,
    "jalan arteri primer": 7,
    "jalan tol": 8
}
ltktnh_mapping = {
    "lain-lain": 1,
    "tusuk sate": 2,
    "normal": 3,
    "hook": 4
}

data['Zonasi'] = data['Zonasi'].astype(str).str.lower().map(zonasi_mapping)
data['Kls_Jalan'] = data['Kls_Jalan'].astype(str).str.lower().map(kls_jalan_mapping)
data['Ltk_Tnh'] = data['Ltk_Tnh'].astype(str).str.lower().map(ltktnh_mapping)

# =============== SPLIT FITUR DAN TARGET =======================
X = data.drop(columns=['ID', 'KOOR_X', 'KOOR_Y', 'ZNT_2024'])
y = data['ZNT_2024'].values.reshape(-1, 1)


# ============== SCALING FITUR DAN TARGET =======================
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y)

# ======================= SPLIT DATA =======================
X_train, X_test, y_train_scaled, y_test_scaled = train_test_split(
    X_scaled, y_scaled, test_size=0.2, random_state=seed
)

# ======================= BANGUN MODEL ANN =======================
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],),
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.3),
    layers.Dense(64,activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.3),
    layers.Dense(32,activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.3),
    layers.Dense(16,activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.3),
    layers.Dense(8,activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.3),
    layers.Dense(1)
])

# ======================= COMPILE MODEL =======================
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# ======================= CALLBACK =======================
early_stop = EarlyStopping(monitor='val_loss',patience=10, restore_best_weights=True)

# ======================= TRAINING =======================
history = model.fit(X_train, y_train_scaled, epochs=100, batch_size=128,
                    validation_split=0.1, verbose=1, callbacks=[early_stop])

# ========== PREDIKSI DAN INVERSE SCALING ====================
y_train_pred_scaled = model.predict(X_train)
y_test_pred_scaled = model.predict(X_test)

# Inverse transform ke skala asli (rupiah)
y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled)
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled)
y_train_real = scaler_y.inverse_transform(y_train_scaled)
y_test_real = scaler_y.inverse_transform(y_test_scaled)

# ======================= EVALUASI =======================
def evaluate(true, pred):
    mse = mean_squared_error(true, pred)
    mae = mean_absolute_error(true, pred)
    r2 = r2_score(true, pred)
    mape = np.mean(np.abs((true.flatten() - pred.flatten()) / true.flatten())) * 100
    acc = 100 - mape
    return mse, mae, mape, acc, r2
mse_train, mae_train, mape_train, acc_train, r2_train = evaluate(y_train_real, y_train_pred)
mse_test, mae_test, mape_test, acc_test, r2_test = evaluate(y_test_real, y_test_pred)

# ======================= HASIL =======================
print("=== Performansi Data Train ===")
print(f'MSE        : {mse_train:.2f}')
print(f'MAE        : {mae_train:.2f}')
print(f'MAPE       : {mape_train:.2f}%')
print(f'Accuracy   : {acc_train:.2f}%')
print(f'R2 Score   : {r2_train:.4f}')

print("\n=== Performansi Data Test ===")
print(f'MSE        : {mse_test:.2f}')
print(f'MAE        : {mae_test:.2f}')
print(f'MAPE       : {mape_test:.2f}%')
print(f'Accuracy   : {acc_test:.2f}%')
print(f'R2 Score   : {r2_test:.4f}')

# ======================= GRAFIK LOSS =======================
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Grafik Loss Value Terhadap Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True)
plt.show()
# ================== PREDIKSI SELURUH DATA =======================
y_all_scaled_pred = model.predict(X_scaled)
y_all_pred = scaler_y.inverse_transform(y_all_scaled_pred)

# Tambahkan hasil prediksi ke dataframe
data['ZNT_2024_PREDIKSI_ANN'] = y_all_pred.flatten()
from sklearn.inspection import permutation_importance

# Gunakan model sebagai fungsi prediksi
def model_predict(X_input):
    pred_scaled = model.predict(X_input)
    return scaler_y.inverse_transform(pred_scaled).flatten()

# Hitung PFI pada data test
pfi_result = permutation_importance(
    estimator=None,             # estimator None karena kita gunakan fungsi custom
    X=X_test,
    y=y_test_real.flatten(),
    scoring='neg_mean_squared_error',
    n_repeats=10,
    random_state=seed,
    n_jobs=-1,
    estimator_predict=model_predict  # custom function
)

# Buat DataFrame PFI
feature_names = X.columns
pfi_df = pd.DataFrame({
    'Fitur': feature_names,
    'Importance': pfi_result['importances_mean']
}).sort_values(by='Importance', ascending=False)

# Tampilkan
print("\n=== Permutation Feature Importance (PFI) ===")
print(pfi_df)